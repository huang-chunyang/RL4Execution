{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b00f89-af12-419b-8251-58b773e4fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Device set to : cpu\n",
      "============================================================================================\n",
      "training environment name : sim_trade\n",
      "current logging run number for sim_trade :  2\n",
      "logging at : PPO_logs/sim_trade//PPO_sim_trade_log_2.csv\n",
      "save checkpoint path : PPO_preTrained/sim_trade/PPO_sim_trade_0_0.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  100000\n",
      "max timesteps per episode :  400\n",
      "model saving frequency : 20000 timesteps\n",
      "log frequency : 800 timesteps\n",
      "printing average reward over episodes in last : 1600 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  7\n",
      "action space dimension :  11\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a discrete action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 1600 timesteps\n",
      "PPO K epochs :  40\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2024-08-13 11:02:56\n",
      "============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "command: \n",
      " ['python', './env/make_env.py', '--inst', '000030.SZ', '--td', '20230829', '--volume', '9205', '--start_time', '93000000', '--end_time', '93930000', '--policy', '/mnt/huangchunyang/RL4Execution/PPO_preTrained/sim_trade/training_PPO_sim_trade_0_0.pth', '--direction', 'sell']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 0it [02:45, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52444.84024972457, 110.0, 640300, 272300, 146.11639196202458, 0.0, 0.0] 7 -1.638455867767334 0.34816962480545044 128717.64622373879\n",
      "[52623.865877712036, 110.0, 710300, 202800, 87.89197915623473, -0.1111, 0.1111111111111111] 4 -2.293360948562622 0.4894809126853943 128717.64622373879\n",
      "[52697.597597597596, 105.0, -1787000, 99900, 53.09190145398826, -0.2222, 0.2222222222222222] 8 -2.1863625049591064 -0.517612099647522 239817.6462237388\n",
      "[52631.1761684643, 100.0, -2246200, 194700, 58.94913061275798, -0.3333, 0.3333333333333333] 6 -2.223736524581909 -0.4946668744087219 17617.64622373879\n",
      "[52426.4450867052, 100.0, -3328000, 69200, 47.69696007084728, -0.4444, 0.4444444444444444] 6 -2.2934205532073975 -0.517612099647522 -93482.35377626121\n",
      "[52454.30847212165, 105.0, -3931900, 138100, 55.84576975922169, -0.5555, 0.5555555555555556] 2 -2.5273401737213135 -0.517612099647522 17617.64622373879\n",
      "[52487.107623318385, 110.0, -2549766, 89200, 25.495097567963924, -0.6666, 0.6666666666666666] 8 -2.1863625049591064 -0.517612099647522 -93482.35377626121\n",
      "[52360.99893730074, 110.0, -3180809, 94100, 75.82875444051551, -0.7777, 0.7777777777777778] 2 -2.5273401737213135 -0.517612099647522 -315682.3537762612\n",
      "[52392.99145299145, 105.0, -2074277, 175500, 93.90819985496474, -0.8888, 0.8888888888888888] 6 -2.223736524581909 -0.4946668744087219 -204582.3537762612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "############################### Import libraries ###############################\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "from env.make_env import TradingEnv\n",
    "import env.config as default_config\n",
    "\n",
    "import gym\n",
    "from net import RolloutBuffer, ActorCritic, PPO\n",
    "# import roboschool\n",
    "# import pybullet_envs\n",
    "\n",
    "\n",
    "################################## set device ##################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "    \n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "\n",
    "\n",
    "####### initialize environment hyperparameters ######\n",
    "\n",
    "env_name = \"sim_trade\"\n",
    "has_continuous_action_space = False\n",
    "\n",
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = None\n",
    "\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "\n",
    "\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "# env = TradingEnv()\n",
    "\n",
    "# state space dimension\n",
    "# state_dim = env.observation_space.shape[0]\n",
    "state_dim = 7\n",
    "# action space dimension\n",
    "\n",
    "# action_dim = env.action_space.n\n",
    "action_dim = 11\n",
    "###################### logging ######################\n",
    "\n",
    "#### log files for multiple runs are NOT overwritten\n",
    "\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + '/' + env_name + '/'\n",
    "if not os.path.exists(log_dir):\n",
    "      os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "\n",
    "#### create new log file for each run \n",
    "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "training_checkpoint_path = directory + \"training_PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"Initializing a discrete action space policy\")\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "if random_seed:\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "#####################################################\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "################# training procedure ################\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, device, action_std)\n",
    "# save initial model\n",
    "ppo_agent.save(training_checkpoint_path)\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "# logging file\n",
    "log_f = open(log_f_name,\"w+\")\n",
    "log_f.write('episode,timestep,reward\\n')\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "from tqdm import tqdm\n",
    "from env.utils import time_delete\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_time_format(time_str):\n",
    "    # 去掉前面的 'z ' 并去掉冒号\n",
    "    time_str = time_str[2:].replace(':', '') + '000'\n",
    "    # 去掉开头的 '0'\n",
    "    return time_str.lstrip('0')\n",
    "def formate_date(date):\n",
    "    '''\n",
    "    input: date(example: '2023/8/29')\n",
    "    output: date(example: '20230829')\n",
    "    '''\n",
    "    date_obj = datetime.strptime(date, '%Y/%m/%d')\n",
    "    formatted_date_str = date_obj.strftime('%Y%m%d')\n",
    "    return formatted_date_str\n",
    "def adjust_row(row):\n",
    "    sample = row[1]\n",
    "    date = formate_date(sample['date'])\n",
    "    sym = sample['sym']\n",
    "    side = sample['side'].lower()\n",
    "    start_time = convert_time_format(sample['start-time'])\n",
    "    end_time = convert_time_format(sample['end-time'])\n",
    "    volume = sample['volume']\n",
    "    return date, sym, side, start_time, end_time, volume\n",
    "    \n",
    "def set_config(default_config, date, sym, side, start_time, end_time, volume):\n",
    "    config = default_config\n",
    "    config.StrategyParam.trading_day = date.replace('-', '')\n",
    "    config.StrategyParam.instrument = sym\n",
    "    config.TradeParam.volume = int(0.05 * volume)\n",
    "    # split_num = int(min(config.TradeParam.volume//100, time_delete(config.TradeParam.end_time, config.TradeParam.start_time)//(3*config.StrategyParam.time_window)))\n",
    "    # config.TradeParam.split_num = split_num\n",
    "    config.TradeParam.start_time = start_time\n",
    "    config.TradeParam.end_time = end_time\n",
    "    return config\n",
    "def adjust_sample(t_result, tot_vwap):\n",
    "    state = t_result[0]['features']\n",
    "    action, action_logprob, state_val = t_result[1]['action'], t_result[1]['action_logprob'], t_result[1]['state_val']\n",
    "    reward_data = t_result[2]\n",
    "    reward = reward_data[0] - reward_data[1]*tot_vwap\n",
    "    return (state, action, action_logprob, state_val, reward)   \n",
    "    \n",
    "# train set \n",
    "train_set = pd.read_csv('./plans/train.csv')\n",
    "for row in tqdm(train_set.iterrows(), desc='training'):\n",
    "    date, sym, side, start_time, end_time, volume = adjust_row(row)\n",
    "    config = set_config(default_config, date, sym, side, start_time, end_time, volume)\n",
    "    command = ['python', './env/make_env.py',  \n",
    "         '--inst', str(config.StrategyParam.instrument), \n",
    "         '--td', str(config.StrategyParam.trading_day), \n",
    "        '--volume', str(config.TradeParam.volume),\n",
    "         '--start_time', str(config.TradeParam.start_time),\n",
    "         '--end_time', str(config.TradeParam.end_time),\n",
    "        '--policy', os.path.abspath(training_checkpoint_path),\n",
    "        '--direction', 'sell' \n",
    "        ]\n",
    "    print(\"command:\", '\\n', command)\n",
    "    result = subprocess.run(command, \n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    output_lines = result.stdout.splitlines()\n",
    "    json_output_lines = []\n",
    "    capture_json = False\n",
    "    for line in output_lines:\n",
    "        if \"JSON_OUTPUT_START\" in line:\n",
    "            capture_json = True\n",
    "            continue\n",
    "        if \"JSON_OUTPUT_END\" in line:\n",
    "            capture_json = False\n",
    "            continue\n",
    "        if capture_json:\n",
    "            json_output_lines.append(line)\n",
    "    \n",
    "    json_output = \"\\n\".join(json_output_lines)\n",
    "    output_data = json.loads(json_output)\n",
    "    train_data = output_data[\"train_data\"]\n",
    "    tot_vwap = output_data[\"tot_vwap\"]\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        t_result = train_data[i]\n",
    "        state, action, action_logprob, state_val, reward = adjust_sample(t_result, tot_vwap)\n",
    "        done = (i == (len(result_output)-1))\n",
    "        print(state, action, action_logprob, state_val, reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e51a0e-d369-4d12-bdea-a1691992aefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [52392.99145299145, 105.0, -2074277, 175500, 93.90819985496474, -0.8888, 0.8888888888888888] tensor([ 5.2393e+04,  1.0500e+02, -2.0743e+06,  1.7550e+05,  9.3908e+01,\n",
      "        -8.8880e-01,  8.8889e-01])\n",
      "torch.float32\n",
      "<class 'int'> 6 tensor(6)\n",
      "torch.int64\n",
      "<class 'float'> -2.223736524581909 tensor(-2.2237)\n",
      "torch.float32\n",
      "<class 'float'> -0.4946668744087219 tensor(-0.4947)\n",
      "torch.float32\n",
      "<class 'float'> -204582.3537762612 tensor(-204582.3594)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for a in [state, action, action_logprob, state_val, reward]:\n",
    "    print(type(a), a, torch.tensor(a))\n",
    "    print(torch.tensor(a).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ebd9e4e-daa7-4ac2-ab81-ddd00f56de63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, tensor(6))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, torch.tensor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b296246d-7407-4899-b689-930c5177777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.5\n",
      "3\n",
      "4.75\n",
      "tensor([1.0000, 2.5000, 3.0000, 4.7500])\n"
     ]
    }
   ],
   "source": [
    "# 假设你有一个包含 int 和 float 的列表\n",
    "data = [1, 2.5, 3, 4.75]\n",
    "\n",
    "# 创建一个空列表来存储单独处理后的 tensors\n",
    "tensor_list = []\n",
    "\n",
    "for item in data:\n",
    "    print(item)\n",
    "    if isinstance(item, int):\n",
    "        tensor_list.append(torch.tensor(item, dtype=torch.int))\n",
    "    elif isinstance(item, float):\n",
    "        tensor_list.append(torch.tensor(item, dtype=torch.float))\n",
    "\n",
    "# 最终将所有张量组合成一个列表形式的张量\n",
    "final_tensor = torch.stack(tensor_list)\n",
    "\n",
    "print(final_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
